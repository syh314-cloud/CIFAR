"""
Dropoutæ­£åˆ™åŒ–å¯¹æ¯”å®éªŒ
å¯¹æ¯”ä¸åŒDropouté…ç½®å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“
ç”Ÿæˆä¸¤ç»„å®éªŒçš„æŸ±çŠ¶å›¾ï¼šéªŒè¯é›†å’Œæµ‹è¯•é›†å‡†ç¡®ç‡å¯¹æ¯”
"""

import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    import cupy as np
    import numpy as np_cpu
except ImportError:
    import numpy as np
    np_cpu = np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import json
import os
from datetime import datetime

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ˜¾ç¤ºå‚æ•°
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.dpi'] = 100
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['font.size'] = 10

from utils.data_loader import train_images, train_labels, val_images, val_labels, test_images, test_labels
from model.mlp_4layer import MLP
from utils.loss import CrossEntropyLoss
from utils.classic_optimizers import Adam

class DropoutExperiment:
    def __init__(self):
        self.SEED = 2023
        np.random.seed(self.SEED)
        
        # åŸºå‡†é…ç½®ï¼ˆä¸L2æ­£åˆ™åŒ–å®éªŒç›¸åŒï¼‰
        self.learning_rate = 7e-5
        self.batch_size = 64
        self.epochs = 100
        
        # ä¸¤ç»„Dropouté…ç½®
        # ç¬¬ä¸€ç»„ï¼šå‰10ä¸ªepochå…³é—­Dropout
        self.group1_configs = {
            'p=(0.1,0.2,0.3)': {'warmup': 10, 'p': (0.1, 0.2, 0.3)},
            'p=(0.2,0.3,0.4)': {'warmup': 10, 'p': (0.2, 0.3, 0.4)},
            'p=(0.3,0.4,0.5)': {'warmup': 10, 'p': (0.3, 0.4, 0.5)},
            'p=(0.2,0.3,0.3)': {'warmup': 10, 'p': (0.2, 0.3, 0.3)},
            'p=(0.2,0.3,0.2)': {'warmup': 10, 'p': (0.2, 0.3, 0.2)},
        }
        
        # ç¬¬äºŒç»„ï¼šå‰20ä¸ªepochå…³é—­Dropout
        self.group2_configs = {
            'p=(0.1,0.2,0.3)': {'warmup': 20, 'p': (0.1, 0.2, 0.3)},
            'p=(0.2,0.3,0.4)': {'warmup': 20, 'p': (0.2, 0.3, 0.4)},
            'p=(0.3,0.4,0.5)': {'warmup': 20, 'p': (0.3, 0.4, 0.5)},
            'p=(0.2,0.3,0.3)': {'warmup': 20, 'p': (0.2, 0.3, 0.3)},
            'p=(0.2,0.3,0.2)': {'warmup': 20, 'p': (0.2, 0.3, 0.2)},
        }
        
        self.results_group1 = {}
        self.results_group2 = {}
    
    def _to_numpy(self, data):
        """å°†CuPyæ•°ç»„è½¬æ¢ä¸ºNumPyæ•°ç»„ç”¨äºmatplotlib"""
        if hasattr(data, 'get'):
            return data.get()
        elif isinstance(data, list):
            return [self._to_numpy(item) for item in data]
        else:
            return data

    def train_single_model(self, config_name, config, group_name):
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        print(f"\nğŸ”§ å¼€å§‹è®­ç»ƒ - {group_name} - {config_name}")
        
        # é‡ç½®éšæœºç§å­ç¡®ä¿å…¬å¹³å¯¹æ¯”
        np.random.seed(self.SEED)
        
        # åˆ›å»ºæ¨¡å‹ï¼ˆä¸L2å®éªŒç›¸åŒçš„ç»“æ„ï¼‰
        model = MLP(train_images.shape[1], 1024, 512, 256, train_labels.shape[1])
        loss_fn = CrossEntropyLoss()
        optimizer = Adam(self.learning_rate, beta1=0.9, beta2=0.999)
        
        warmup_epochs = config['warmup']
        dropout_p = config['p']
        
        step_losses = []
        val_accuracies = []
        best_val_acc = 0.0
        
        for epoch in range(self.epochs):
            # è®¾ç½®dropoutç­–ç•¥
            if epoch < warmup_epochs:
                # Warmupé˜¶æ®µï¼šå…³é—­Dropout
                model.dropout1.p = 0.0
                model.dropout2.p = 0.0
                model.dropout3.p = 0.0
            else:
                # æ­£å¼è®­ç»ƒï¼šä½¿ç”¨æŒ‡å®šçš„Dropoutæ¦‚ç‡
                model.dropout1.p = dropout_p[0]
                model.dropout2.p = dropout_p[1]
                model.dropout3.p = dropout_p[2]
            
            np.random.seed(self.SEED + epoch)
            idx = np.random.permutation(train_images.shape[0])
            shuffled_images = train_images[idx]
            shuffled_labels = train_labels[idx]
            
            epoch_losses = []
            
            for i in range(0, shuffled_images.shape[0], self.batch_size):
                x = shuffled_images[i:i+self.batch_size]
                y = shuffled_labels[i:i+self.batch_size]
                
                model.zero_grad()
                y_pred = model.forward(x, training=True)
          
                # ä¸ä½¿ç”¨L2æ­£åˆ™åŒ–
                loss = loss_fn.forward(y_pred, y, model, lambda_l2=0.0)
                step_losses.append(loss)
                epoch_losses.append(loss)
                
                grad_output = loss_fn.backward()
                model.backward(grad_output)
                
                optimizer.step(model)
            
            # éªŒè¯
            val_pred = model.forward(val_images, training=False)
            val_acc = np.mean(np.argmax(val_pred, axis=1) == np.argmax(val_labels, axis=1))
            val_accuracies.append(val_acc)
            
            # è½¬æ¢ä¸ºæ ‡é‡
            if hasattr(val_acc, 'get'):
                val_acc_scalar = float(val_acc.get())
            else:
                val_acc_scalar = float(val_acc)
            
            if val_acc_scalar > best_val_acc:
                best_val_acc = val_acc_scalar
            
            avg_loss = np.mean(np.array(epoch_losses))
            
            if (epoch + 1) % 10 == 0:
                print(f"  Epoch {epoch+1:3d}/{self.epochs} - Loss: {avg_loss:.4f}, Val Acc: {val_acc_scalar:.4f}, Best Val: {best_val_acc:.4f}")
        
        # æµ‹è¯•
        test_pred = model.forward(test_images, training=False)
        test_acc = np.mean(np.argmax(test_pred, axis=1) == np.argmax(test_labels, axis=1))
        
        # è½¬æ¢ä¸ºæ ‡é‡
        if hasattr(test_acc, 'get'):
            test_acc_scalar = float(test_acc.get())
        else:
            test_acc_scalar = float(test_acc)
        
        print(f"âœ… è®­ç»ƒå®Œæˆ - æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}, æµ‹è¯•å‡†ç¡®ç‡: {test_acc_scalar:.4f}")
        
        return {
            'step_losses': self._to_numpy(step_losses),
            'val_accuracies': self._to_numpy(val_accuracies),
            'best_val_accuracy': best_val_acc,
            'test_accuracy': test_acc_scalar,
            'warmup_epochs': warmup_epochs,
            'dropout_p': dropout_p,
            'learning_rate': self.learning_rate,
            'batch_size': self.batch_size
        }
    
    def run_experiments(self):
        """è¿è¡Œæ‰€æœ‰å®éªŒ"""
        print(f"\n{'='*80}")
        print(f"å¼€å§‹ Dropout æ­£åˆ™åŒ–å¯¹æ¯”å®éªŒ")
        print(f"{'='*80}")
        print(f"é…ç½®ä¿¡æ¯:")
        print(f"  - å­¦ä¹ ç‡: {self.learning_rate}")
        print(f"  - Batch Size: {self.batch_size}")
        print(f"  - Epochs: {self.epochs}")
        print(f"  - âŒ å·²å…³é—­L2æ­£åˆ™åŒ–å’Œæ•°æ®å¢å¼º")
        print(f"{'='*80}\n")
        
        # ç¬¬ä¸€ç»„å®éªŒï¼šå‰10ä¸ªepochå…³é—­Dropout
        print(f"\n{'='*80}")
        print(f"ç¬¬ä¸€ç»„å®éªŒï¼šå‰ 10 ä¸ª epoch å…³é—­ Dropout")
        print(f"{'='*80}")
        for config_name, config in self.group1_configs.items():
            result = self.train_single_model(config_name, config, "Group1")
            self.results_group1[config_name] = result
        
        # ç¬¬äºŒç»„å®éªŒï¼šå‰20ä¸ªepochå…³é—­Dropout
        print(f"\n{'='*80}")
        print(f"ç¬¬äºŒç»„å®éªŒï¼šå‰ 20 ä¸ª epoch å…³é—­ Dropout")
        print(f"{'='*80}")
        for config_name, config in self.group2_configs.items():
            result = self.train_single_model(config_name, config, "Group2")
            self.results_group2[config_name] = result
        
        # ä¿å­˜ç»“æœ
        self.save_results()
        
        # ç»˜åˆ¶å›¾è¡¨
        self.plot_results()
    
    def save_results(self):
        """ä¿å­˜å®éªŒç»“æœåˆ°JSON"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f'dropout_comparison_results_{timestamp}.json'
        
        # é¢„å¤„ç†ç»“æœï¼Œç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯å¯åºåˆ—åŒ–çš„
        def process_results(results_dict):
            processed = {}
            for config_name, result in results_dict.items():
                processed[config_name] = {
                    'step_losses': [float(x) if hasattr(x, 'get') else float(x) for x in result['step_losses']],
                    'val_accuracies': [float(x) if hasattr(x, 'get') else float(x) for x in result['val_accuracies']],
                    'best_val_accuracy': float(result['best_val_accuracy']),
                    'test_accuracy': float(result['test_accuracy']),
                    'warmup_epochs': result['warmup_epochs'],
                    'dropout_p': result['dropout_p'],
                    'learning_rate': float(result['learning_rate']),
                    'batch_size': int(result['batch_size'])
                }
            return processed
        
        results = {
            'group1': process_results(self.results_group1),
            'group2': process_results(self.results_group2)
        }
        
        with open(filename, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {filename}")
    
    def plot_results(self):
        """ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨"""
        print(f"\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        # åˆ›å»ºä¸¤ä¸ªå­å›¾
        plt.close('all')
        fig, axes = plt.subplots(1, 2, figsize=(20, 8))
        fig.suptitle('Dropout Regularization Comparison: Validation & Test Accuracy', 
                    fontsize=18, fontweight='bold', y=0.98)
        
        # ç»˜åˆ¶ç¬¬ä¸€ç»„ç»“æœ
        self._plot_group(axes[0], self.results_group1, 
                        'Group 1: Warmup 10 Epochs (Dropout OFF) â†’ Dropout ON',
                        'Warmup=10')
        
        # ç»˜åˆ¶ç¬¬äºŒç»„ç»“æœ
        self._plot_group(axes[1], self.results_group2, 
                        'Group 2: Warmup 20 Epochs (Dropout OFF) â†’ Dropout ON',
                        'Warmup=20')
        
        # è°ƒæ•´å¸ƒå±€
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        
        # ä¿å­˜å›¾è¡¨
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f'dropout_comparison_plots_{timestamp}.png'
        plt.savefig(filename, dpi=300, bbox_inches='tight', 
                   facecolor='white', edgecolor='none')
        
        print(f"âœ… å›¾è¡¨å·²ä¿å­˜åˆ°: {filename}")
        
        # æ‰“å°ç»“æœæ€»ç»“
        self._print_summary()
    
    def _plot_group(self, ax, results, title, group_label):
        """ç»˜åˆ¶å•ä¸ªç»„çš„æŸ±çŠ¶å›¾"""
        config_names = list(results.keys())
        val_accs = [results[name]['best_val_accuracy'] for name in config_names]
        test_accs = [results[name]['test_accuracy'] for name in config_names]
        
        # è®¾ç½®æŸ±çŠ¶å›¾å‚æ•°
        x = self._to_numpy(np.arange(len(config_names)))
        width = 0.35
        
        # ç»˜åˆ¶æŸ±çŠ¶å›¾
        bars1 = ax.bar(x - width/2, val_accs, width, label='Validation Accuracy',
                      color='#4ECDC4', alpha=0.8, edgecolor='black', linewidth=1.5)
        bars2 = ax.bar(x + width/2, test_accs, width, label='Test Accuracy',
                      color='#FF6B6B', alpha=0.8, edgecolor='black', linewidth=1.5)
        
        # æ·»åŠ æ ‡ç­¾å’Œæ ‡é¢˜
        ax.set_xlabel('Dropout Configuration', fontsize=13, fontweight='bold')
        ax.set_ylabel('Accuracy', fontsize=13, fontweight='bold')
        ax.set_title(title, fontsize=14, fontweight='bold', pad=15)
        ax.set_xticks(x)
        ax.set_xticklabels(config_names, fontsize=10, rotation=15, ha='right')
        ax.legend(fontsize=11, loc='lower right')
        ax.grid(True, alpha=0.3, axis='y', linestyle='--')
        ax.tick_params(axis='both', which='major', labelsize=10)
        
        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar in bars1:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                   f'{height:.4f}', ha='center', va='bottom', 
                   fontweight='bold', fontsize=8)
        
        for bar in bars2:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                   f'{height:.4f}', ha='center', va='bottom', 
                   fontweight='bold', fontsize=8)
        
        # è®¾ç½®yè½´èŒƒå›´
        y_max = max(max(val_accs), max(test_accs))
        ax.set_ylim(0, y_max * 1.15)
    
    def _print_summary(self):
        """æ‰“å°ç»“æœæ€»ç»“"""
        print(f"\n{'='*80}")
        print(f"å®éªŒç»“æœæ€»ç»“")
        print(f"{'='*80}")
        
        print(f"\nç¬¬ä¸€ç»„ (Warmup 10 Epochs):")
        for name, result in self.results_group1.items():
            print(f"  {name:20s}: Val Acc={result['best_val_accuracy']:.4f}, Test Acc={result['test_accuracy']:.4f}")
        
        print(f"\nç¬¬äºŒç»„ (Warmup 20 Epochs):")
        for name, result in self.results_group2.items():
            print(f"  {name:20s}: Val Acc={result['best_val_accuracy']:.4f}, Test Acc={result['test_accuracy']:.4f}")
        
        # æ‰¾å‡ºæœ€ä½³é…ç½®
        all_results = {**self.results_group1, **self.results_group2}
        best_config = max(all_results.items(), key=lambda x: x[1]['test_accuracy'])
        print(f"\nğŸ† æœ€ä½³é…ç½®: {best_config[0]}")
        print(f"   éªŒè¯å‡†ç¡®ç‡: {best_config[1]['best_val_accuracy']:.4f}")
        print(f"   æµ‹è¯•å‡†ç¡®ç‡: {best_config[1]['test_accuracy']:.4f}")
        print(f"   Warmup Epochs: {best_config[1]['warmup_epochs']}")
        print(f"   Dropout p: {best_config[1]['dropout_p']}")
        print(f"{'='*80}\n")

def main():
    experiment = DropoutExperiment()
    experiment.run_experiments()

if __name__ == '__main__':
    main()

