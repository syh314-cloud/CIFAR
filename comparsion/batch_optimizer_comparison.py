"""
Batch Optimizerå¯¹æ¯”å®éªŒ
æ‰¹é‡è®­ç»ƒæ¨¡å‹ï¼Œå¯¹æ¯”ä¸åŒæ‰¹æ¬¡ä¼˜åŒ–å™¨çš„æ•ˆæœ
ç”Ÿæˆè®­ç»ƒLossæ›²çº¿ã€éªŒè¯Accuracyæ›²çº¿å’Œæµ‹è¯•å‡†ç¡®ç‡æŸ±çŠ¶å›¾
"""

import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    import cupy as np
    import numpy as np_cpu
except ImportError:
    import numpy as np
    np_cpu = np
import matplotlib
matplotlib.use('Agg')  # è®¾ç½®åç«¯ï¼Œé¿å…GUIé—®é¢˜
import matplotlib.pyplot as plt
import json
import os
from datetime import datetime
from collections import OrderedDict

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ˜¾ç¤ºå‚æ•°
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.dpi'] = 100
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['font.size'] = 10

from utils.data_loader import train_images, train_labels, val_images, val_labels, test_images, test_labels
from model.mlp_4layer import MLP
from utils.loss import CrossEntropyLoss, L2Scheduler
from utils.batch_optimizers import BatchGD, OnlineGD, MiniBatchGD, AdaptiveBatchGD
from utils.data_augmentation import augment_images

class BatchOptimizerExperiment:
    def __init__(self):
        self.SEED = 2023
        np.random.seed(self.SEED)
        
        # åŸºå‡†batch size
        self.batch_size = 64
        
        # ä¸ºæ¯ä¸ªä¼˜åŒ–å™¨è®¾ç½®åˆç†çš„å­¦ä¹ ç‡ï¼Œè®©å®ƒä»¬éƒ½åœ¨"åˆç†å·¥ä½œç‚¹"
        # MiniBatchGD (batch=64)ï¼š7e-5 - ç¬¬ä¸€ä¸ªè®­ç»ƒ
        # BatchGD (full batch)ï¼š3e-4 - ç¬¬äºŒä¸ªè®­ç»ƒ
        # AdaptiveBatchGDï¼š1e-4 - ç¬¬ä¸‰ä¸ªè®­ç»ƒ
        # OnlineGD (batch=1)ï¼š3e-4 - ç¬¬å››ä¸ªè®­ç»ƒ
        # ä½¿ç”¨ OrderedDict ä¿è¯é¡ºåº
        self.optimizers_config = OrderedDict([
            ('MiniBatchGD', {'class': MiniBatchGD, 'params': {'lr': 7e-5}}),
            ('BatchGD', {'class': BatchGD, 'params': {'lr': 3e-4}}),
            ('AdaptiveBatchGD', {'class': AdaptiveBatchGD, 'params': {'lr': 1e-4}}),
            ('OnlineGD', {'class': OnlineGD, 'params': {'lr': 3e-4}}),
        ])
        
        self.results = {}
        self.epochs = 100
    
    def _to_numpy(self, data):
        """å°†CuPyæ•°ç»„è½¬æ¢ä¸ºNumPyæ•°ç»„ç”¨äºmatplotlib"""
        if hasattr(data, 'get'):  # CuPyæ•°ç»„
            return data.get()
        elif isinstance(data, list):
            return [self._to_numpy(item) for item in data]
        else:
            return data

    def train_single_model(self, optimizer_name, optimizer_config):
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        print(f"\nğŸ”§ å¼€å§‹è®­ç»ƒ - Optimizer: {optimizer_name}")
        
        # é‡ç½®éšæœºç§å­ç¡®ä¿å…¬å¹³å¯¹æ¯”
        np.random.seed(self.SEED)
        
        # åˆ›å»ºæ¨¡å‹
        model = MLP(train_images.shape[1], 1024, 512, 256, train_labels.shape[1])
        loss_fn = CrossEntropyLoss()
        l2_scheduler = L2Scheduler(base_lambda=1e-4)
        
        # åˆ›å»ºä¼˜åŒ–å™¨å®ä¾‹
        optimizer = optimizer_config['class'](**optimizer_config['params'])
        
        lambda_l2 = l2_scheduler.base_lambda
        
        step_losses = []  # è®°å½•æ¯ä¸ªstepçš„loss
        val_accuracies = []
        
        # åˆå§‹batch size
        if optimizer_name == 'OnlineGD':
            current_batch_size = 1  # Online GDæ¯æ¬¡åªç”¨ä¸€ä¸ªæ ·æœ¬
        elif optimizer_name == 'BatchGD':
            current_batch_size = train_images.shape[0]  # Batch GDä½¿ç”¨å…¨éƒ¨æ•°æ®
        else:  # MiniBatchGD å’Œ AdaptiveBatchGD
            current_batch_size = self.batch_size
        
        steps_per_epoch = 0  # ç”¨äºç»Ÿè®¡å¹³å‡æ¯epochçš„æ­¥æ•°
        
        for epoch in range(self.epochs):
            # AdaptiveBatchGD: åœ¨æ¯ä¸ªepochå¼€å§‹æ—¶æ›´æ–°batch size
            if optimizer_name == 'AdaptiveBatchGD' and epoch > 0:
                current_batch_size = optimizer.get_adaptive_batch_size()
                print(f"    Epoch {epoch}: Adaptive batch size = {current_batch_size}")
            
            # è®¾ç½®dropoutç­–ç•¥ - ä»epoch 10å¼€å§‹å¯ç”¨
            if epoch < 10:
                model.dropout1.p = 0.0
                model.dropout2.p = 0.0
                model.dropout3.p = 0.0
            else:
                model.dropout1.p = 0.2
                model.dropout2.p = 0.3
                model.dropout3.p = 0.3
            
            np.random.seed(self.SEED + epoch)
            idx = np.random.permutation(train_images.shape[0])
            shuffled_images = train_images[idx]
            shuffled_labels = train_labels[idx]
            
            epoch_losses = []
            epoch_steps = 0
            
            # BatchGDç‰¹æ®Šå¤„ç†ï¼šç´¯ç§¯æ•´ä¸ªepochçš„æ¢¯åº¦
            if optimizer_name == 'BatchGD':
                optimizer.reset()  # é‡ç½®ç´¯ç§¯çš„æ¢¯åº¦
                # BatchGDéœ€è¦éå†æ‰€æœ‰mini-batchæ¥ç´¯ç§¯æ¢¯åº¦
                mini_batch_size = 64  # ä½¿ç”¨å°æ‰¹é‡æ¥éå†æ•°æ®
                for i in range(0, shuffled_images.shape[0], mini_batch_size):
                    epoch_steps += 1
                    x = shuffled_images[i:i+mini_batch_size]
                    y = shuffled_labels[i:i+mini_batch_size]
                    
                    x = x.reshape(-1, 3, 32, 32)
                    x = augment_images(x, seed=self.SEED + epoch * 1000 + i)
                    x = x.reshape(x.shape[0], -1)
                    
                    model.zero_grad()
                    y_pred = model.forward(x, training=True)
              
                    loss = loss_fn.forward(y_pred, y, model, lambda_l2=lambda_l2)
                    step_losses.append(loss)  # è®°å½•æ¯ä¸ªstepçš„loss
                    epoch_losses.append(loss)
                    
                    grad_output = loss_fn.backward()
                    model.backward(grad_output)
                    
                    for layer in model.layers:
                        if hasattr(layer, 'w'):
                            layer.dw += lambda_l2 * layer.w
                    
                    optimizer.accumulate_gradients(model)
                
                # BatchGDåœ¨epochç»“æŸåç»Ÿä¸€æ›´æ–°
                optimizer.step(model)
            else:
                # å…¶ä»–ä¼˜åŒ–å™¨æ­£å¸¸è®­ç»ƒ
                for i in range(0, shuffled_images.shape[0], current_batch_size):
                    epoch_steps += 1
                    x = shuffled_images[i:i+current_batch_size]
                    y = shuffled_labels[i:i+current_batch_size]
                    
                    x = x.reshape(-1, 3, 32, 32)
                    x = augment_images(x, seed=self.SEED + epoch * 1000 + i)
                    x = x.reshape(x.shape[0], -1)
                    
                    model.zero_grad()
                    y_pred = model.forward(x, training=True)
              
                    loss = loss_fn.forward(y_pred, y, model, lambda_l2=lambda_l2)
                    step_losses.append(loss)  # è®°å½•æ¯ä¸ªstepçš„loss
                    epoch_losses.append(loss)
                    
                    grad_output = loss_fn.backward()
                    model.backward(grad_output)
                    
                    for layer in model.layers:
                        if hasattr(layer, 'w'):
                            layer.dw += lambda_l2 * layer.w
                    
                    optimizer.step(model)
            
            # ç»Ÿè®¡å¹³å‡æ­¥æ•°
            steps_per_epoch += epoch_steps
            
            val_pred = model.forward(val_images, training=False)
            val_acc = np.mean(np.argmax(val_pred, axis=1) == np.argmax(val_labels, axis=1))
            val_accuracies.append(val_acc)
            
            avg_loss = np.mean(np.array(epoch_losses))
            
            if (epoch + 1) % 10 == 0:
                print(f"  Epoch {epoch+1:2d}/{self.epochs} - Loss: {avg_loss:.4f}, Val Acc: {val_acc:.4f}")
        
        test_pred = model.forward(test_images, training=False)
        test_acc = np.mean(np.argmax(test_pred, axis=1) == np.argmax(test_labels, axis=1))
        
        print(f"âœ… è®­ç»ƒå®Œæˆ - æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
        
        # è®¡ç®—å¹³å‡æ¯epochçš„æ­¥æ•°
        avg_steps_per_epoch = steps_per_epoch // self.epochs if self.epochs > 0 else 0
        
        return {
            'step_losses': step_losses,  # æ¯ä¸ªstepçš„loss
            'val_accuracies': val_accuracies,
            'test_accuracy': test_acc,
            'optimizer_name': optimizer_name,
            'steps_per_epoch': avg_steps_per_epoch,
            'actual_batch_size': current_batch_size if optimizer_name != 'AdaptiveBatchGD' else 'Adaptive'
        }
    
    def run_experiments(self):
        """è¿è¡Œæ‰€æœ‰å®éªŒ"""
        print(f"\nğŸ¯ å¼€å§‹Batch Optimizerå¯¹æ¯”å®éªŒ...")
        print(f"ğŸ“¦ åŸºå‡†Batch Size: {self.batch_size}")
        print(f"ğŸ”§ æµ‹è¯•ä¼˜åŒ–å™¨åŠå…¶å­¦ä¹ ç‡:")
        for opt_name, opt_config in self.optimizers_config.items():
            lr = opt_config['params']['lr']
            print(f"   â€¢ {opt_name}: {lr}")
        print(f"ğŸ’§ Dropoutå¯ç”¨æ—¶é—´: epoch >= 10")
        print("-" * 60)
        
        for i, (opt_name, opt_config) in enumerate(self.optimizers_config.items()):
            print(f"\n{'='*60}")
            print(f"å®éªŒè¿›åº¦: {i+1}/{len(self.optimizers_config)}")
            
            result = self.train_single_model(opt_name, opt_config)
            self.results[opt_name] = result
        
        print(f"\nğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆï¼")
        self.save_results()
        self.plot_results()
    
    def save_results(self):
        """ä¿å­˜å®éªŒç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"batch_optimizer_comparison_results_{timestamp}.json"
        
        # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
        results_to_save = {}
        for opt_name, result in self.results.items():
            step_losses = [float(x.get() if hasattr(x, 'get') else x) for x in result['step_losses']]
            val_accuracies = [float(x.get() if hasattr(x, 'get') else x) for x in result['val_accuracies']]
            test_accuracy = float(result['test_accuracy'].get() if hasattr(result['test_accuracy'], 'get') else result['test_accuracy'])
            # ä»ä¼˜åŒ–å™¨é…ç½®ä¸­è·å–å­¦ä¹ ç‡
            learning_rate = self.optimizers_config[opt_name]['params']['lr']
            results_to_save[opt_name] = {
                'step_losses': step_losses,
                'val_accuracies': val_accuracies,
                'test_accuracy': test_accuracy,
                'optimizer_name': result['optimizer_name'],
                'steps_per_epoch': result['steps_per_epoch'],
                'actual_batch_size': result['actual_batch_size'],
                'learning_rate': learning_rate,
                'batch_size': self.batch_size
            }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results_to_save, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {filename}")
    
    def plot_results(self):
        """ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨"""
        print(f"\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        # åˆ›å»ºå›¾è¡¨
        plt.close('all')  # å…³é—­ä¹‹å‰çš„å›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Batch Optimizer Comparison Results', fontsize=18, fontweight='bold', y=0.98)
        
        # å®šä¹‰é¢œè‰²æ–¹æ¡ˆ - ä½¿ç”¨æ›´æ˜“åŒºåˆ†çš„é¢œè‰²
        colors = {
            'BatchGD': '#FF6B6B',        # çº¢è‰²
            'OnlineGD': '#4ECDC4',       # é’è‰²
            'MiniBatchGD': '#FFA500',    # æ©™è‰²
            'AdaptiveBatchGD': '#9B59B6' # ç´«è‰²
        }
        
        # 1. è®­ç»ƒLossæ›²çº¿ (æŒ‰epochï¼Œæ¯ä¸ªepochå–å¹³å‡loss)
        ax1 = axes[0, 0]
        for opt_name, result in self.results.items():
            step_losses_np = self._to_numpy(result['step_losses'])
            steps_per_epoch = result['steps_per_epoch']
            
            # è®¡ç®—æ¯ä¸ªepochçš„å¹³å‡loss
            epoch_losses = []
            for epoch in range(self.epochs):
                start_idx = epoch * steps_per_epoch
                end_idx = min((epoch + 1) * steps_per_epoch, len(step_losses_np))
                if start_idx < len(step_losses_np):
                    epoch_loss = np.mean(step_losses_np[start_idx:end_idx])
                    epoch_losses.append(epoch_loss)
            
            epochs = range(len(epoch_losses))
            ax1.plot(epochs, epoch_losses, label=opt_name, 
                    color=colors.get(opt_name, 'gray'), linewidth=2, alpha=0.8)
        ax1.set_title('Training Loss Curves (Average per Epoch)', fontsize=14, fontweight='bold', pad=15)
        ax1.set_xlabel('Epoch', fontsize=12)
        ax1.set_ylabel('Loss', fontsize=12)
        ax1.set_xlim(left=0)  # ç¡®ä¿Xè½´ä»0å¼€å§‹
        ax1.legend(fontsize=10, loc='upper right')
        ax1.grid(True, alpha=0.3)
        ax1.tick_params(axis='both', which='major', labelsize=10)
        
        # 2. éªŒè¯Accuracyæ›²çº¿
        ax2 = axes[0, 1]
        for opt_name, result in self.results.items():
            val_accuracies_np = self._to_numpy(result['val_accuracies'])
            ax2.plot(val_accuracies_np, label=opt_name, 
                    color=colors.get(opt_name, 'gray'), linewidth=2, marker='s', markersize=3)
        ax2.set_title('Validation Accuracy Curves', fontsize=14, fontweight='bold', pad=15)
        ax2.set_xlabel('Epoch', fontsize=12)
        ax2.set_ylabel('Accuracy', fontsize=12)
        ax2.legend(fontsize=10, loc='lower right')
        ax2.grid(True, alpha=0.3)
        ax2.tick_params(axis='both', which='major', labelsize=10)
        
        # 3. æµ‹è¯•å‡†ç¡®ç‡æŸ±çŠ¶å›¾
        ax3 = axes[1, 0]
        opt_names = list(self.results.keys())
        test_accs = [self._to_numpy(self.results[name]['test_accuracy']) for name in opt_names]
        
        bar_colors = [colors.get(name, 'gray') for name in opt_names]
        bars = ax3.bar(range(len(opt_names)), test_accs, 
                      color=bar_colors, alpha=0.8, edgecolor='black', linewidth=1)
        ax3.set_title('Final Test Accuracy Comparison', fontsize=14, fontweight='bold', pad=15)
        ax3.set_xlabel('Optimizer', fontsize=12)
        ax3.set_ylabel('Test Accuracy', fontsize=12)
        ax3.set_xticks(range(len(opt_names)))
        ax3.set_xticklabels(opt_names, fontsize=10)
        ax3.grid(True, alpha=0.3, axis='y')
        ax3.tick_params(axis='both', which='major', labelsize=10)
        
        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, acc) in enumerate(zip(bars, test_accs)):
            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
                    f'{acc:.4f}', ha='center', va='bottom', fontweight='bold')
        
        # 4. æœ€ç»ˆæ”¶æ•›æ€§èƒ½å¯¹æ¯”è¡¨æ ¼
        ax4 = axes[1, 1]
        ax4.axis('tight')
        ax4.axis('off')
        
        # å‡†å¤‡è¡¨æ ¼æ•°æ®
        table_data = []
        headers = ['Optimizer', 'å®é™…Batch Size', 'æœ€ç»ˆLoss', 'æœ€ä½³Val Acc', 'æµ‹è¯•å‡†ç¡®ç‡']
        
        for opt_name in opt_names:
            result = self.results[opt_name]
            actual_bs = result['actual_batch_size']
            final_loss = self._to_numpy(result['step_losses'][-1])  # æœ€åä¸€ä¸ªstepçš„loss
            best_val_acc = max(self._to_numpy(result['val_accuracies']))
            test_acc = self._to_numpy(result['test_accuracy'])
            
            table_data.append([
                opt_name,
                f'{actual_bs}',
                f'{final_loss:.4f}',
                f'{best_val_acc:.4f}',
                f'{test_acc:.4f}'
            ])
        
        # åˆ›å»ºè¡¨æ ¼
        table = ax4.table(cellText=table_data, colLabels=headers,
                         cellLoc='center', loc='center',
                         colWidths=[0.2, 0.2, 0.2, 0.2, 0.2])
        table.auto_set_font_size(False)
        table.set_fontsize(9)
        table.scale(1, 2)
        
        # è®¾ç½®è¡¨æ ¼æ ·å¼
        for i in range(len(headers)):
            table[(0, i)].set_facecolor('#4ECDC4')
            table[(0, i)].set_text_props(weight='bold', color='white')
        
        ax4.set_title('Performance Summary', fontsize=14, fontweight='bold', pad=20)
        
        # è°ƒæ•´å¸ƒå±€
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # ä¸ºsuptitleç•™å‡ºç©ºé—´
        
        # ä¿å­˜å›¾è¡¨
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f'batch_optimizer_comparison_plots_{timestamp}.png'
        plt.savefig(filename, dpi=300, bbox_inches='tight', 
                   facecolor='white', edgecolor='none')
        
        print(f"ğŸ“ˆ å›¾è¡¨å·²ä¿å­˜åˆ°: {filename}")
        
        # å°è¯•æ˜¾ç¤ºå›¾è¡¨ï¼ˆå¦‚æœæœ‰GUIç¯å¢ƒï¼‰
        try:
            plt.show()
        except:
            print("ğŸ“± æ— GUIç¯å¢ƒï¼Œå›¾è¡¨å·²ä¿å­˜ä¸ºæ–‡ä»¶")
        
        # æ‰“å°æœ€ä½³ç»“æœ
        best_opt = max(self.results.keys(), key=lambda name: self._to_numpy(self.results[name]['test_accuracy']))
        best_acc = self._to_numpy(self.results[best_opt]['test_accuracy'])
        print(f"\nğŸ† æœ€ä½³ä¼˜åŒ–å™¨: {best_opt} (æµ‹è¯•å‡†ç¡®ç‡: {best_acc:.4f})")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ Batch Optimizerå¯¹æ¯”å®éªŒ")
    print("=" * 60)
    
    # åˆ›å»ºå®éªŒå¯¹è±¡
    experiment = BatchOptimizerExperiment()
    
    # è¿è¡Œå®éªŒ
    experiment.run_experiments()
    
    print("\nâœ… å®éªŒå®Œæˆï¼")
    print("ğŸ“Š å·²ç”Ÿæˆ:")
    print("  - è®­ç»ƒLossæ›²çº¿ (æŒ‰æ­¥æ•°)")
    print("  - éªŒè¯Accuracyæ›²çº¿") 
    print("  - æµ‹è¯•å‡†ç¡®ç‡æŸ±çŠ¶å›¾")
    print("  - æ€§èƒ½å¯¹æ¯”è¡¨æ ¼")

if __name__ == "__main__":
    main()
